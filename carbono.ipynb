{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This example no longer works since v6, use v5 or prior to use it.\nfrom IPython.display import FileLink, display\nimport torch.nn as torch_nn\nfrom torch.utils.data import DataLoader\nimport numpy as np\nimport os, subprocess, time, json, torch\n\ndef download(download_file_name):\n    os.chdir(f\"/kaggle/working/\")\n    name = f\"{download_file_name}.uai\"\n    display(FileLink(f'{name}'))\n\nclass carbono:\n    def __init__(self, debug=True):\n        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n        self.layers = []\n        self.model = None\n        self.debug = debug\n        self.labels = None\n\n    def save_pytorch(self, filename='model'):\n        filename = filename + '.pt'\n        \"\"\"Save model in PyTorch format\"\"\"\n        torch.save(self.model.state_dict(), filename)\n    \n    def load_pytorch(self, filename='model.pt'):\n        \"\"\"Load model from PyTorch format\"\"\"\n        if self.model is None:\n            raise ValueError(\"Model architecture must be defined before loading weights\")\n        self.model.load_state_dict(torch.load(filename))\n    \n    def layer(self, input_size, output_size, activation='tanh'):\n        \"\"\"Add a layer to the network, similar to carbono.js\"\"\"\n        self.layers.append({\n            'input_size': input_size,\n            'output_size': output_size,\n            'activation': activation\n        })\n\n        # Check if layers are compatible\n        if len(self.layers) > 1:\n            prev_layer = self.layers[-2]\n            if prev_layer['output_size'] != input_size:\n                raise ValueError(f\"Layer input size {input_size} doesn't match previous layer output size {prev_layer['output_size']}\")\n\n        # Build/rebuild model when layer is added\n        self._build_model()\n        \n        if self.debug:\n            print(f\"Added layer: {input_size} → {output_size} with {activation} activation\")\n\n    def _build_model(self):\n        \"\"\"Build PyTorch model from layers\"\"\"\n        if not self.layers:\n            return\n    \n        layers = []\n        for i, layer_info in enumerate(self.layers):\n            # Add linear layer\n            layers.append(torch_nn.Linear(layer_info['input_size'], layer_info['output_size']))\n            \n            # Add activation, but skip softmax for the last layer\n            if i < len(self.layers) - 1:  # Only add activation for non-final layers\n                if layer_info['activation'] == 'tanh':\n                    layers.append(torch_nn.Tanh())\n                elif layer_info['activation'] == 'relu':\n                    layers.append(torch_nn.ReLU())\n                elif layer_info['activation'] == 'sigmoid':\n                    layers.append(torch_nn.Sigmoid())\n    \n        self.model = torch_nn.Sequential(*layers).to(self.device)\n\n    def train(self, train_set, options=None):\n        if options is None:\n            options = {}\n    \n        # Default options similar to carbono.js\n        epochs = options.get('epochs', 200)\n        learning_rate = options.get('learningRate', 0.212)\n        print_every_epochs = options.get('printEveryEpochs', 10)\n        early_stop_threshold = options.get('earlyStopThreshold', 1e-6)\n        optimizer_type = options.get('optimizer', 'adam')\n        loss_function = options.get('lossFunction', 'cross-entropy')\n    \n        # Convert data to PyTorch format\n        if isinstance(train_set[0]['output'], str):\n            unique_labels = list(set(item['output'] for item in train_set))\n            self.labels = unique_labels\n            \n            num_classes = len(unique_labels)\n            label_to_idx = {label: i for i, label in enumerate(unique_labels)}\n            \n            x_data = torch.tensor([item['input'] for item in train_set], dtype=torch.float32).to(self.device)\n            # Change this part - use class indices instead of one-hot encoding\n            y_data = torch.tensor([label_to_idx[item['output']] for item in train_set], dtype=torch.long).to(self.device)\n        else:\n            x_data = torch.tensor([item['input'] for item in train_set], dtype=torch.float32).to(self.device)\n            y_data = torch.tensor([item['output'] for item in train_set], dtype=torch.float32).to(self.device)\n    \n        # Create DataLoader\n        dataset = torch.utils.data.TensorDataset(x_data, y_data)\n        train_loader = DataLoader(dataset, batch_size=32, shuffle=True)  # Remove generator parameter\n    \n        # Rest of the training code remains the same\n        if loss_function == 'mse':\n            criterion = torch_nn.MSELoss()\n        elif loss_function == 'cross-entropy':\n            criterion = torch_nn.CrossEntropyLoss()\n    \n        if optimizer_type == 'adam':\n            optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)\n        else:\n            optimizer = torch.optim.SGD(self.model.parameters(), lr=learning_rate)\n    \n        start_time = time.time()\n    \n        for epoch in range(epochs):\n            total_loss = 0\n            for inputs, targets in train_loader:\n                optimizer.zero_grad()\n                outputs = self.model(inputs)\n                loss = criterion(outputs, targets)\n                loss.backward()\n                optimizer.step()\n                \n                total_loss += loss.item()\n    \n            avg_loss = total_loss / len(train_loader)\n            \n            if (epoch + 1) % print_every_epochs == 0 and self.debug:\n                print(f'Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.6f}')\n    \n            if avg_loss < early_stop_threshold:\n                if self.debug:\n                    print(f'Early stopping at epoch {epoch+1} with loss: {avg_loss:.6f}')\n                break\n    \n        training_time = (time.time() - start_time) * 1000\n    \n        total_params = sum(p.numel() for p in self.model.parameters())\n        self.model.details = {\n            'loss': avg_loss,\n            'parameters': total_params,\n            'training': {\n                'time': training_time,\n                'epochs': epoch + 1,\n                'learningRate': learning_rate\n            },\n            'layers': self.layers\n        }\n        return self.model.details\n\n    def predict(self, input_data, tags=True):\n        \"\"\"Make predictions similar to carbono.js\"\"\"\n        with torch.no_grad():\n            input_tensor = torch.tensor(input_data, dtype=torch.float32).to(self.device)\n            if len(input_tensor.shape) == 1:\n                input_tensor = input_tensor.unsqueeze(0)\n            \n            output = self.model(input_tensor)\n            predictions = output.cpu().numpy()\n\n            if self.labels and tags:\n                # Return labeled probabilities like carbono.js\n                return [\n                    {\n                        'label': self.labels[i],\n                        'probability': float(prob)\n                    }\n                    for i, prob in enumerate(predictions[0])\n                ]\n            \n            return predictions[0].tolist()\n\n    def save(self, filename='model'):\n        filename = filename + '.uai'\n        \"\"\"Export model in carbono.js format\"\"\"\n        carbono_model = {\n            'weights': [],\n            'biases': [],\n            'layers': self.layers,\n            'labels': self.labels,\n            'details': self.model.details\n        }\n        \n        current_layer = None\n        for layer in self.model:\n            if isinstance(layer, torch_nn.Linear):\n                weights = layer.weight.detach().cpu().numpy().tolist()\n                biases = layer.bias.detach().cpu().numpy().tolist()\n                carbono_model['weights'].append(weights)\n                carbono_model['biases'].append(biases)\n\n        with open(filename, 'w') as f:\n            json.dump(carbono_model, f)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-25T20:19:48.565987Z","iopub.execute_input":"2024-11-25T20:19:48.566401Z","iopub.status.idle":"2024-11-25T20:19:51.927135Z","shell.execute_reply.started":"2024-11-25T20:19:48.566363Z","shell.execute_reply":"2024-11-25T20:19:51.925836Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Create a more complex dataset with 4 inputs and 3 outputs\ntrain_set = [\n    {'input': [1, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'output': '😀'},  # Smiling Face\n    {'input': [0, 1, 0, 0, 0, 0, 0, 0, 0, 0], 'output': '😊'},  # Smiling Face with Smiling Eyes\n    {'input': [0, 0, 1, 0, 0, 0, 0, 0, 0, 0], 'output': '😂'},  # Face with Tears of Joy\n    {'input': [0, 0, 0, 1, 0, 0, 0, 0, 0, 0], 'output': '😍'},  # Smiling Face with Heart-Eyes\n    {'input': [0, 0, 0, 0, 1, 0, 0, 0, 0, 0], 'output': '😎'},  # Smiling Face with Sunglasses\n    {'input': [0, 0, 0, 0, 0, 1, 0, 0, 0, 0], 'output': '😢'},  # Crying Face\n    {'input': [0, 0, 0, 0, 0, 0, 1, 0, 0, 0], 'output': '😡'},  # Pouting Face\n    {'input': [0, 0, 0, 0, 0, 0, 0, 1, 0, 0], 'output': '😴'},  # Sleeping Face\n    {'input': [0, 0, 0, 0, 0, 0, 0, 0, 1, 0], 'output': '🤔'},  # Thinking Face\n    {'input': [0, 0, 0, 0, 0, 0, 0, 0, 0, 1], 'output': '🤢'},  # Nauseated Face\n]\n\n# Create and train model\nnn = carbono(debug=True)\n\n# Add layers with specified architecture\nnn.layer(10, 128, 'sigmoid')\nnn.layer(128, 512, 'relu')\nnn.layer(512, 128, 'relu')\nnn.layer(128, 10, 'softmax')  # Output layer (3 outputs for 3 classes)\n\n# Train the model with adjusted parameters\ntraining_summary = nn.train(train_set, {\n    'epochs': 1000,\n    'learningRate': 0.0002,\n    'printEveryEpochs': 100,\n    'optimizer': 'adam',\n    'lossFunction': 'cross-entropy'\n})\n\n# Export and download model\nmodel_name = \"model\"\nnn.save(model_name)\ndownload(model_name)\n# gpu time: 4317.9385 ~4 seconds\n# cpu time: 144691.6623 ~2.5 minutes","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Vocabulary (tokens)\nvocab = {\n    '<PAD>': 0,  # Padding token\n    '<EOS>': 1,  # End of string token\n    'hello': 2,\n    'world': 3,\n    'how': 4,\n    'are': 5,\n    'you': 6,\n    'today': 7,\n    'nice': 8,\n    'weather': 9,\n}\n# Reverse vocabulary for token to word conversion\nreverse_vocab = {v: k for k, v in vocab.items()}\n\ntrain_set = [\n    # \"hello world\"\n    {'input': [1, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'output': 'world'},\n    {'input': [0, 1, 0, 0, 0, 0, 0, 0, 0, 0], 'output': 'EOS'},\n    \n    # \"how are you\"\n    {'input': [0, 0, 1, 0, 0, 0, 0, 0, 0, 0], 'output': 'are'},\n    {'input': [0, 0, 0, 1, 0, 0, 0, 0, 0, 0], 'output': 'you'},\n    {'input': [0, 0, 0, 0, 1, 0, 0, 0, 0, 0], 'output': 'EOS'},\n    \n    # \"nice weather today\"\n    {'input': [0, 0, 0, 0, 0, 1, 0, 0, 0, 0], 'output': 'weather'},\n    {'input': [0, 0, 0, 0, 0, 0, 1, 0, 0, 0], 'output': 'today'},\n    {'input': [0, 0, 0, 0, 0, 0, 0, 1, 0, 0], 'output': 'EOS'}\n]\n    \n# Create and train model\nnn = carbono(debug=True)\n# Add layers with specified architecture\nnn.layer(10, 128, 'tanh')\nnn.layer(128, 6, 'softmax')  # Output layer\n\n# Train the model with adjusted parameters\ntraining_summary = nn.train(train_set, {\n    'epochs': 40,\n    'learningRate': 0.001,\n    'printEveryEpochs': 10,\n    'optimizer': 'adam',\n    'lossFunction': 'cross-entropy',\n    'earlyStopThreshold': 1e-9\n})\n\n# Export and download model\nmodel_name = \"labels_test1\"\nnn.save(model_name)\ndownload(model_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T14:21:17.713499Z","iopub.execute_input":"2024-11-25T14:21:17.714161Z","iopub.status.idle":"2024-11-25T14:21:19.092459Z","shell.execute_reply.started":"2024-11-25T14:21:17.714098Z","shell.execute_reply":"2024-11-25T14:21:19.091373Z"}},"outputs":[{"name":"stdout","text":"Added layer: 10 → 128 with tanh activation\nAdded layer: 128 → 6 with softmax activation\nEpoch [10/40], Loss: 1.529232\nEpoch [20/40], Loss: 1.325388\nEpoch [30/40], Loss: 1.138497\nEpoch [40/40], Loss: 0.964135\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"/kaggle/working/labels_test1.uai","text/html":"<a href='labels_test1.uai' target='_blank'>labels_test1.uai</a><br>"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"rm ./*","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Sample text with longer sequences\ntext = \"\"\"\nthe quick brown fox jumps over the lazy dog\nshe sells seashells by the seashore\nhow much wood would a woodchuck chuck if a woodchuck could chuck wood\nto be or not to be that is the question\nall that glitters is not gold\na journey of a thousand miles begins with a single step\n\"\"\"\n\n# Preprocess the text\nsentences = [line.strip().split() for line in text.splitlines() if line.strip()]\n\n# Automatically create vocabulary from the text\nvocab = {'<PAD>': 0, '<EOS>': 1}  # Start with special tokens\nword_set = set(word for sentence in sentences for word in sentence)\nfor i, word in enumerate(sorted(word_set)):\n    vocab[word] = i + 2  # +2 because we already have PAD and EOS\n\n# Assign values between 0 and 1 to each token\nnum_tokens = len(vocab)\ntoken_values = {token: (id + 1) / (num_tokens + 1) for token, id in vocab.items()}\n\n# Append '<EOS>' to each sentence\nsentences = [sentence + ['<EOS>'] for sentence in sentences]\n\n# Determine the context window size (max sentence length)\ncontext_window = max(len(sentence) for sentence in sentences)\n\n# Initialize the training set\ntrain_set = []\n\n# Generate training examples\nfor sentence in sentences:\n    # Only generate examples up to the actual sentence length (including <EOS>)\n    for i in range(len(sentence)-1):  # -1 because we don't need to predict after <EOS>\n        # Create input with tokens on the left and padding on the right\n        input_seq = sentence[:i+1] + ['<PAD>'] * (context_window - (i+1))\n        # Map tokens to their values\n        input_values = [token_values[token] for token in input_seq]\n        # Output is the next token\n        output_token = sentence[i+1]\n        train_set.append({'input': input_values, 'output': output_token})\n\n# Print vocabulary\nprint(\"Vocabulary:\")\nfor token, id in vocab.items():\n    print(f\"{token}: {id} (value: {token_values[token]:.3f})\")\n\nprint(\"\\nSample of training set:\")\n# Print first 10 examples\nfor example in train_set[:10]:\n    print(f\"\\nInput sequence: {example['input']}\")\n    print(f\"Target token: {example['output']}\")\n    \nprint(f\"\\nTotal number of training examples: {len(train_set)}\")\nprint(f\"Context window size: {context_window}\")\nprint(f\"Vocabulary size: {len(vocab)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nfrom collections import defaultdict\n\nclass TextTokenizer:\n    def __init__(self, max_length=50):\n        self.vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n        self.reverse_vocab = {0: \"<PAD>\", 1: \"<UNK>\"}\n        self.vocab_size = 2\n        self.max_length = max_length\n    \n    def fit(self, texts):\n        # Build vocabulary from texts\n        for text in texts:\n            for token in text.split():\n                if token not in self.vocab:\n                    self.vocab[token] = self.vocab_size\n                    self.reverse_vocab[self.vocab_size] = token\n                    self.vocab_size += 1\n    \n    def encode(self, text):\n        # Convert text to padded sequence of token IDs\n        tokens = text.split()\n        encoded = [self.vocab.get(token, self.vocab[\"<UNK>\"]) for token in tokens]\n        # Pad sequence\n        padded = encoded + [self.vocab[\"<PAD>\"]] * (self.max_length - len(encoded))\n        return padded[:self.max_length]\n    \n    def normalize_tokens(self, token_ids):\n        # Normalize token IDs to range [0,1]\n        return [id / self.vocab_size for id in token_ids]\n\n# Example usage:\ntexts = [\n    \"the cat sat on the mat\",\n    \"dogs are friendly pets\",\n    \"birds fly in the sky\"\n]\n\n# Create training set\ndef create_training_set(texts, max_length=50):\n    tokenizer = TextTokenizer(max_length=max_length)\n    tokenizer.fit(texts)\n    \n    train_set = []\n    for text in texts:\n        tokens = tokenizer.encode(text)\n        normalized_tokens = tokenizer.normalize_tokens(tokens)\n        \n        # For each position, predict the next token\n        for i in range(len(tokens) - 1):\n            input_sequence = normalized_tokens[:i+1] + [0] * (max_length - (i+1))\n            target_token = tokens[i+1]\n            \n            train_set.append({\n                'input': input_sequence,\n                'output': target_token\n            })\n    \n    return train_set, tokenizer\n\n# Create and prepare the dataset\ntrain_set, tokenizer = create_training_set(texts)\n\n# Model architecture (similar to your example but adjusted for text)\nnn = carbono(debug=True)\nnn.layer(50, 256, 'relu')  # Input size = max_length\nnn.layer(256, 512, 'relu')\nnn.layer(512, 256, 'relu')\nnn.layer(256, tokenizer.vocab_size, 'softmax')  # Output size = vocab_size\n\n# Train the model\ntraining_summary = nn.train(train_set, {\n    'epochs': 1000,\n    'learningRate': 0.0002,\n    'printEveryEpochs': 100,\n    'optimizer': 'adam',\n    'lossFunction': 'cross-entropy'\n})\n\n# Save model\nnn.save(\"text_model\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Model architecture (similar to your example but adjusted for text)\nnn = carbono(debug=True)\nnn.layer(10, 256, 'tanh')\nnn.layer(256, 2, 'sigmoid') \n\ntrain_set = [\n    # \"hello world\"\n    {'input': [1, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'output': [0,1]},\n    {'input': [0, 1, 0, 0, 0, 0, 0, 0, 0, 0], 'output': [0,1]},\n    {'input': [0, 0, 1, 0, 0, 0, 0, 0, 0, 0], 'output': [1,0]},\n    {'input': [0, 0, 0, 1, 0, 0, 0, 0, 0, 0], 'output': [1,0]},\n]\n\n# Train the model\ntraining_summary = nn.train(train_set, {\n    'epochs': 1000,\n    'learningRate': 0.001,\n    'printEveryEpochs': 100,\n    'optimizer': 'adam'\n})\n\n# Save model\nnn.save(\"numbers2\")\ndownload(\"numbers2\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T20:28:45.427725Z","iopub.execute_input":"2024-11-25T20:28:45.428133Z","iopub.status.idle":"2024-11-25T20:28:46.797186Z","shell.execute_reply.started":"2024-11-25T20:28:45.428094Z","shell.execute_reply":"2024-11-25T20:28:46.796104Z"}},"outputs":[{"name":"stdout","text":"Added layer: 10 → 256 with tanh activation\nAdded layer: 256 → 2 with sigmoid activation\nEpoch [100/1000], Loss: 0.019082\nEpoch [200/1000], Loss: 0.004546\nEpoch [300/1000], Loss: 0.002052\nEpoch [400/1000], Loss: 0.001175\nEpoch [500/1000], Loss: 0.000763\nEpoch [600/1000], Loss: 0.000536\nEpoch [700/1000], Loss: 0.000397\nEpoch [800/1000], Loss: 0.000306\nEpoch [900/1000], Loss: 0.000243\nEpoch [1000/1000], Loss: 0.000197\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"/kaggle/working/numbers2.uai","text/html":"<a href='numbers2.uai' target='_blank'>numbers2.uai</a><br>"},"metadata":{}}],"execution_count":16}]}
