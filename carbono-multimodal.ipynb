{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from IPython.display import FileLink, display\nimport torch.nn as torch_nn\nfrom torch.utils.data import DataLoader\nimport numpy as np\nimport os, subprocess, time, json, torch\nimport struct\n\nimport requests\nfrom PIL import Image\nimport io\nimport librosa\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ndef download(download_file_name):\n    os.chdir(f\"/kaggle/working/\")\n    name = f\"{download_file_name}.uai\"\n    display(FileLink(f'{name}'))\n\nclass carbono:\n    def __init__(self, debug=True):\n        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n        self.layers = []\n        self.model = None\n        self.debug = debug\n        self.labels = None\n        self.details = {}\n\n    def _is_url(self, input_str):\n        \"\"\"Check if input is a URL\"\"\"\n        try:\n            from urllib.parse import urlparse\n            result = urlparse(input_str)\n            return all([result.scheme, result.netloc])\n        except:\n            return False\n\n    def _infer_content_type(self, url, content_type=None):\n        \"\"\"Infer content type from URL or content-type header\"\"\"\n        if content_type and 'binary/octet-stream' not in content_type:\n            if content_type.startswith('image/'): return 'image'\n            if content_type.startswith('audio/'): return 'audio'\n            if content_type.startswith('text/'): return 'text'\n\n        extension = url.split('.')[-1].lower()\n        if extension in ['jpg', 'jpeg', 'png', 'gif', 'bmp', 'webp']:\n            return 'image'\n        elif extension in ['wav', 'mp3', 'ogg', 'aac', 'flac']:\n            return 'audio'\n        elif extension in ['txt', 'csv', 'json', 'html', 'xml']:\n            return 'text'\n\n        raise ValueError(f\"Unable to infer content type for {url}\")\n\n    def _preprocess_image(self, content):\n        \"\"\"Preprocess image data\"\"\"\n        img = Image.open(io.BytesIO(content))\n        img = img.resize((32, 32))\n        img = img.convert('RGB')\n        img_array = np.array(img)\n        normalized = img_array.reshape(-1) / 255.0\n        return normalized\n\n    def _preprocess_audio(self, content):\n        \"\"\"Preprocess audio data\"\"\"\n        try:\n            y, sr = librosa.load(io.BytesIO(content), sr=44100, duration=5)\n            # Extract mel spectrogram features\n            mel_spect = librosa.feature.melspectrogram(y=y, sr=sr)\n            mel_spect_db = librosa.power_to_db(mel_spect, ref=np.max)\n            # Flatten and normalize\n            normalized = (mel_spect_db - mel_spect_db.min()) / (mel_spect_db.max() - mel_spect_db.min())\n            return normalized.flatten()\n        except Exception as e:\n            raise ValueError(f\"Error preprocessing audio: {str(e)}\")\n\n    def _preprocess_text(self, content):\n        \"\"\"Preprocess text data\"\"\"\n        text = content.decode('utf-8')\n        vectorizer = TfidfVectorizer(max_features=1024)\n        tfidf_matrix = vectorizer.fit_transform([text])\n        return tfidf_matrix.toarray()[0]\n\n    def _pad_or_truncate(self, data, target_size):\n        \"\"\"Pad or truncate data to target size\"\"\"\n        if len(data) > target_size:\n            return data[:target_size]\n        elif len(data) < target_size:\n            return np.pad(data, (0, target_size - len(data)))\n        return data\n\n    async def preprocess_data(self, input_data):\n        \"\"\"Preprocess input data based on type\"\"\"\n        if isinstance(input_data, str) and self._is_url(input_data):\n            try:\n                response = requests.get(input_data)\n                response.raise_for_status()\n                content_type = response.headers.get('Content-Type', '')\n                inferred_type = self._infer_content_type(input_data, content_type)\n\n                if inferred_type == 'image':\n                    processed = self._preprocess_image(response.content)\n                elif inferred_type == 'audio':\n                    processed = self._preprocess_audio(response.content)\n                elif inferred_type == 'text':\n                    processed = self._preprocess_text(response.content)\n                else:\n                    raise ValueError(f\"Unsupported data type for {input_data}\")\n\n                # Get target size from first layer if available\n                target_size = self.layers[0]['input_size'] if self.layers else len(processed)\n                return self._pad_or_truncate(processed, target_size)\n\n            except Exception as e:\n                raise ValueError(f\"Error preprocessing data from {input_data}: {str(e)}\")\n        return input_data\n\n    async def train_from_urls(self, train_set_urls, options=None):\n        \"\"\"Train model using URLs as input\"\"\"\n        processed_train_set = []\n        for data in train_set_urls:\n            try:\n                processed_input = await self.preprocess_data(data['url'])\n                processed_train_set.append({\n                    'input': processed_input,\n                    'output': data['output']\n                })\n            except Exception as e:\n                if self.debug:\n                    print(f\"Error processing {data['url']}: {str(e)}\")\n                continue\n\n        return self.train(processed_train_set, options)\n    \n    def save_pytorch(self, filename='model'):\n        filename = filename + '.pt'\n        \"\"\"Save model in PyTorch format\"\"\"\n        torch.save(self.model.state_dict(), filename)\n    \n    def load_pytorch(self, filename='model.pt'):\n        \"\"\"Load model from PyTorch format\"\"\"\n        if self.model is None:\n            raise ValueError(\"Model architecture must be defined before loading weights\")\n        self.model.load_state_dict(torch.load(filename))\n    \n    def layer(self, input_size, output_size, activation='tanh'):\n        \"\"\"Add a layer to the network, similar to carbono.js\"\"\"\n        self.layers.append({\n            'input_size': input_size,\n            'output_size': output_size,\n            'activation': activation\n        })\n\n        # Check if layers are compatible\n        if len(self.layers) > 1:\n            prev_layer = self.layers[-2]\n            if prev_layer['output_size'] != input_size:\n                raise ValueError(f\"Layer input size {input_size} doesn't match previous layer output size {prev_layer['output_size']}\")\n\n        # Build/rebuild model when layer is added\n        self._build_model()\n        \n        if self.debug:\n            print(f\"Added layer: {input_size} → {output_size} with {activation} activation\")\n\n    def _build_model(self):\n        \"\"\"Build PyTorch model from layers\"\"\"\n        if not self.layers:\n            return\n    \n        layers = []\n        for i, layer_info in enumerate(self.layers):\n            # Add linear layer\n            layers.append(torch_nn.Linear(layer_info['input_size'], layer_info['output_size']))\n            \n            # Add activation, but skip softmax for the last layer\n            if i < len(self.layers) - 1:  # Only add activation for non-final layers\n                if layer_info['activation'] == 'tanh':\n                    layers.append(torch_nn.Tanh())\n                elif layer_info['activation'] == 'relu':\n                    layers.append(torch_nn.ReLU())\n                elif layer_info['activation'] == 'sigmoid':\n                    layers.append(torch_nn.Sigmoid())\n    \n        self.model = torch_nn.Sequential(*layers).to(self.device)\n\n    def train(self, train_set, options=None):\n        if options is None:\n            options = {}\n    \n        # Default options\n        epochs = options.get('epochs', 200)\n        learning_rate = options.get('learningRate', 0.212)\n        print_every_epochs = options.get('printEveryEpochs', 10)\n        early_stop_threshold = options.get('earlyStopThreshold', 1e-6)\n        optimizer_type = options.get('optimizer', 'adam')\n        loss_function = options.get('lossFunction', 'cross-entropy')\n    \n        # Ensure train_set is a list\n        if not isinstance(train_set, list):\n            train_set = list(train_set)\n    \n        # Convert data to PyTorch format and handle labels\n        if isinstance(train_set[0]['output'], str):\n            # Get unique labels while preserving order of first appearance\n            seen = set()\n            self.labels = [x['output'] for x in train_set \n                          if not (x['output'] in seen or seen.add(x['output']))]\n            label_to_idx = {label: i for i, label in enumerate(self.labels)}\n            \n            x_data = torch.tensor([item['input'] for item in train_set], dtype=torch.float32).to(self.device)\n            y_data = torch.tensor([label_to_idx[item['output']] for item in train_set], dtype=torch.long).to(self.device)\n            \n            if self.debug:\n                print(f\"Training with {len(self.labels)} classes: {self.labels}\")\n        else:\n            x_data = torch.tensor([item['input'] for item in train_set], dtype=torch.float32).to(self.device)\n            y_data = torch.tensor([item['output'] for item in train_set], dtype=torch.float32).to(self.device)\n            self.labels = None\n    \n        # Create DataLoader\n        dataset = torch.utils.data.TensorDataset(x_data, y_data)\n        train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n    \n        # Loss function\n        if loss_function == 'mse':\n            criterion = torch_nn.MSELoss()\n        elif loss_function == 'cross-entropy':\n            criterion = torch_nn.CrossEntropyLoss()\n    \n        # Optimizer\n        if optimizer_type == 'adam':\n            optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)\n        else:\n            optimizer = torch.optim.SGD(self.model.parameters(), lr=learning_rate)\n    \n        start_time = time.time()\n    \n        for epoch in range(epochs):\n            total_loss = 0\n            for inputs, targets in train_loader:\n                optimizer.zero_grad()\n                outputs = self.model(inputs)\n                loss = criterion(outputs, targets)\n                loss.backward()\n                optimizer.step()\n                \n                total_loss += loss.item()\n    \n            avg_loss = total_loss / len(train_loader)\n            \n            if (epoch + 1) % print_every_epochs == 0 and self.debug:\n                print(f'Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.6f}')\n    \n            if avg_loss < early_stop_threshold:\n                if self.debug:\n                    print(f'Early stopping at epoch {epoch+1} with loss: {avg_loss:.6f}')\n                break\n    \n        training_time = (time.time() - start_time) * 1000\n    \n        total_params = sum(p.numel() for p in self.model.parameters())\n        self.details = {\n            'loss': avg_loss,\n            'parameters': total_params,\n            'training': {\n                'time': training_time,\n                'epochs': epoch + 1,\n                'learningRate': learning_rate\n            },\n            'layers': self.layers\n        }\n    \n        if self.debug:\n            print(f\"Training completed. Model has {len(self.labels)} classes: {self.labels}\")\n        \n        return self.details\n        \n    async def predict(self, input_data, tags=True):\n        \"\"\"Make predictions with support for URL inputs\"\"\"\n        try:\n            # Preprocess the input if it's a URL or raw data\n            processed_input = await self.preprocess_data(input_data)\n            \n            # Convert to tensor and ensure correct shape\n            with torch.no_grad():\n                input_tensor = torch.tensor(processed_input, dtype=torch.float32).to(self.device)\n                if len(input_tensor.shape) == 1:\n                    input_tensor = input_tensor.unsqueeze(0)\n                \n                output = self.model(input_tensor)\n                \n                # Apply softmax for classification\n                if self.labels is not None:\n                    probabilities = torch.nn.functional.softmax(output, dim=1)\n                    predictions = probabilities.cpu().numpy()[0]\n                    \n                    # # Debug output\n                    # if self.debug:\n                    #     print(f\"Raw predictions shape: {predictions.shape}\")\n                    #     print(f\"Number of labels: {len(self.labels)}\")\n                    #     print(f\"Predictions: {predictions}\")\n                    #     print(f\"Labels: {self.labels}\")\n                    \n                    if tags:\n                        # Ensure predictions match number of labels\n                        predictions = predictions[:len(self.labels)]\n                        # Return labeled probabilities sorted by probability\n                        result = [\n                            {\n                                'label': self.labels[i],\n                                'probability': float(predictions[i])\n                            }\n                            for i in range(len(self.labels))\n                        ]\n                        return sorted(result, key=lambda x: x['probability'], reverse=True)\n                    \n                    return predictions.tolist()\n                \n                # For regression, return raw outputs\n                return output.cpu().numpy()[0].tolist()\n                \n        except Exception as e:\n            if self.debug:\n                print(f\"Error during prediction: {str(e)}\")\n                print(f\"Shape of output: {output.shape}\")\n                print(f\"Number of labels: {len(self.labels) if self.labels else 'No labels'}\")\n                raise\n            \n    def save(self, filename='model'):\n        filename = filename + '.uai'\n        \"\"\"Export model in carbono.js format\"\"\"\n        carbono_model = {\n            'weights': [],\n            'biases': [],\n            'layers': self.layers,\n            'labels': self.labels,\n            'details': self.details\n        }\n        \n        current_layer = None\n        for layer in self.model:\n            if isinstance(layer, torch_nn.Linear):\n                weights = layer.weight.detach().cpu().numpy().tolist()\n                biases = layer.bias.detach().cpu().numpy().tolist()\n                carbono_model['weights'].append(weights)\n                carbono_model['biases'].append(biases)\n\n        # Convert weights and biases to binary format\n        weight_bin = b''.join([struct.pack('f', w) for layer in carbono_model['weights'] for row in layer for w in row])\n        bias_bin = b''.join([struct.pack('f', b) for layer in carbono_model['biases'] for b in layer])\n\n        # Prepare metadata\n        metadata = {\n            'layers': self.layers,\n            'details': self.details,\n            'layerInfo': {\n                'weightShapes': [list(map(len, [layer, layer[0]])) for layer in carbono_model['weights']],\n                'biasShapes': [len(layer) for layer in carbono_model['biases']]\n            },\n            'labels': self.labels\n        }\n\n        # Combine metadata and binary data\n        metadata_str = json.dumps(metadata)\n        separator = b'\\n---BINARY_SEPARATOR---\\n'\n        binary_data = metadata_str.encode('utf-8') + separator + weight_bin + bias_bin\n\n        # Save to file\n        with open(filename, 'wb') as f:\n            f.write(binary_data)\n\n    def load(self, filename):\n        \"\"\"Load model from carbono.js format\"\"\"\n        with open(filename, 'rb') as f:\n            data = f.read()\n\n        # Find separator\n        separator = b'\\n---BINARY_SEPARATOR---\\n'\n        sep_index = data.find(separator)\n        if sep_index == -1:\n            raise ValueError(\"Invalid file format\")\n\n        # Extract metadata and binary data\n        metadata_str = data[:sep_index].decode('utf-8')\n        binary_data = data[sep_index + len(separator):]\n\n        # Parse metadata\n        metadata = json.loads(metadata_str)\n        self.layers = metadata['layers']\n        self.details = metadata['details']\n        self.labels = metadata.get('labels', None)\n\n        # Rebuild model\n        self._build_model()\n\n        # Extract weights and biases\n        weight_shapes = metadata['layerInfo']['weightShapes']\n        bias_shapes = metadata['layerInfo']['biasShapes']\n\n        # Reconstruct weights and biases\n        weight_size = sum(shape[0] * shape[1] for shape in weight_shapes)\n        bias_size = sum(shape for shape in bias_shapes)\n\n        weights = struct.unpack('f' * weight_size, binary_data[:weight_size * 4])\n        biases = struct.unpack('f' * bias_size, binary_data[weight_size * 4:])\n\n        # Assign weights and biases to model\n        weight_index = 0\n        bias_index = 0\n        layer_index = 0\n        for layer in self.model:\n            if isinstance(layer, torch_nn.Linear):\n                # Assign weights\n                weight_shape = weight_shapes[layer_index]\n                weight_values = weights[weight_index:weight_index + weight_shape[0] * weight_shape[1]]\n                weight_tensor = torch.tensor(weight_values, dtype=torch.float32).reshape(weight_shape[0], weight_shape[1])\n                layer.weight.data = weight_tensor.to(self.device)\n                weight_index += weight_shape[0] * weight_shape[1]\n\n                # Assign biases\n                bias_shape = bias_shapes[layer_index]\n                bias_values = biases[bias_index:bias_index + bias_shape]\n                bias_tensor = torch.tensor(bias_values, dtype=torch.float32)\n                layer.bias.data = bias_tensor.to(self.device)\n                bias_index += bias_shape\n\n                layer_index += 1\n\n        if self.debug:\n            print(\"Model loaded successfully!\")\n\n    def info(self, info_updates):\n        \"\"\"Update model metadata\"\"\"\n        if 'info' not in self.details:\n            self.details['info'] = {}\n        self.details['info'].update(info_updates)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-06T20:41:40.254388Z","iopub.execute_input":"2025-01-06T20:41:40.254812Z","iopub.status.idle":"2025-01-06T20:41:40.584808Z","shell.execute_reply.started":"2025-01-06T20:41:40.254779Z","shell.execute_reply":"2025-01-06T20:41:40.583495Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"model = carbono()\nmodel.layer(1024, 24, 'sigmoid')\nmodel.layer(24, 4, 'softmax')\n\ntrain_set_urls = [\n    {'url': 'https://cdn.pixabay.com/photo/2018/03/31/06/31/dog-3277416_1280.jpg', 'output': 'dog image'},\n    {'url': 'https://cdn.pixabay.com/photo/2014/11/30/14/11/cat-551554_1280.jpg', 'output': 'cat image'},\n    {'url': 'https://cdn.jsdelivr.net/gh/lunu-bounir/audio-equalizer/test/left.ogg', 'output': 'left audio'},\n    {'url': 'https://raw.githubusercontent.com/appvoid/carbono/refs/heads/main/examples.md', 'output': 'markdown text'},\n]\n\nawait model.train_from_urls(train_set_urls, {\n    'epochs': 10,\n    'optimizer': 'adam',\n    'learningRate': 0.1,\n    'printEveryEpochs': 1\n})\n\n# Iterate over each item in the train_set_urls\nfor item in train_set_urls:\n    url = item['url']\n    expected_output = item['output']\n    \n    # Predict the output for the current URL\n    result = await model.predict(url)\n    \n    # Print the prediction along with the expected output\n    print(f'Expected Output: {expected_output}')\n    print(f'Prediction: {result}')\n    print('---')\n\n# Export and download model\nmodel_name = \"multimodal\"\nmodel.save(model_name)\ndownload(model_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T20:48:38.348173Z","iopub.execute_input":"2025-01-06T20:48:38.348572Z","iopub.status.idle":"2025-01-06T20:48:39.112295Z","shell.execute_reply.started":"2025-01-06T20:48:38.348544Z","shell.execute_reply":"2025-01-06T20:48:39.110427Z"}},"outputs":[{"name":"stdout","text":"Added layer: 1024 → 24 with sigmoid activation\nAdded layer: 24 → 4 with softmax activation\nTraining with 4 classes: ['dog image', 'cat image', 'left audio', 'markdown text']\nEpoch [1/10], Loss: 1.436800\nEpoch [2/10], Loss: 1.398230\nEpoch [3/10], Loss: 0.947650\nEpoch [4/10], Loss: 0.709429\nEpoch [5/10], Loss: 0.595756\nEpoch [6/10], Loss: 0.472842\nEpoch [7/10], Loss: 0.351180\nEpoch [8/10], Loss: 0.240556\nEpoch [9/10], Loss: 0.153561\nEpoch [10/10], Loss: 0.094467\nTraining completed. Model has 4 classes: ['dog image', 'cat image', 'left audio', 'markdown text']\nExpected Output: dog image\nPrediction: [{'label': 'dog image', 'probability': 0.9542640447616577}, {'label': 'left audio', 'probability': 0.03245037421584129}, {'label': 'markdown text', 'probability': 0.010960397310554981}, {'label': 'cat image', 'probability': 0.00232517602853477}]\n---\nExpected Output: cat image\nPrediction: [{'label': 'cat image', 'probability': 0.9913267493247986}, {'label': 'dog image', 'probability': 0.004002803470939398}, {'label': 'markdown text', 'probability': 0.003791898488998413}, {'label': 'left audio', 'probability': 0.0008785000536590815}]\n---\nExpected Output: left audio\nPrediction: [{'label': 'left audio', 'probability': 0.8831714391708374}, {'label': 'markdown text', 'probability': 0.06062605232000351}, {'label': 'dog image', 'probability': 0.05169224739074707}, {'label': 'cat image', 'probability': 0.004510288126766682}]\n---\nExpected Output: markdown text\nPrediction: [{'label': 'markdown text', 'probability': 0.947655200958252}, {'label': 'left audio', 'probability': 0.04606633260846138}, {'label': 'cat image', 'probability': 0.004051519092172384}, {'label': 'dog image', 'probability': 0.002226910088211298}]\n---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"/kaggle/working/multimodal.uai","text/html":"<a href='multimodal.uai' target='_blank'>multimodal.uai</a><br>"},"metadata":{}}],"execution_count":29}]}